{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45b4a159",
   "metadata": {},
   "source": [
    "> **Note:** This notebook contains only code.  \n",
    "> All outputs, plots, and data-derived tensors from the Sleep Heart Health Study (SHHS) dataset have been removed  \n",
    "> to comply with the NSRR data use agreement.  \n",
    "> To reproduce results, please obtain the dataset from [https://sleepdata.org/datasets/shhs](https://sleepdata.org/datasets/shhs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9996dfa8-dcbd-41f6-8932-6893c3b0343a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys, platform, subprocess, time, warnings, logging, pathlib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tf_keras_vis.utils.model_modifiers import ReplaceToLinear\n",
    "from tf_keras_vis.utils.scores import CategoricalScore\n",
    "from tf_keras_vis.gradcam import Gradcam, GradcamPlusPlus\n",
    "from tf_keras_vis.saliency import Saliency\n",
    "from tf_explain.core.integrated_gradients import IntegratedGradients\n",
    "import shap\n",
    "import lime.lime_tabular as lime_tab\n",
    "\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"shap\")\n",
    "logging.getLogger(\"shap\").setLevel(logging.ERROR)\n",
    "\n",
    "np.random.seed(99)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] Libraries imported.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244d46f5-4736-4039-809b-1cef37edaa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(cmd):\n",
    "    try:\n",
    "        return subprocess.check_output(cmd, shell=True, text=True).strip()\n",
    "    except Exception:\n",
    "        return \"N/A\"\n",
    "\n",
    "# -- CPU-Bezeichnung --\n",
    "cpu_model = run(\"lscpu | grep -m1 'Model name' | cut -d ':' -f2- | xargs\")\n",
    "if cpu_model == \"N/A\":\n",
    "    cpu_model = run(\"grep -m1 'model name' /proc/cpuinfo | cut -d ':' -f2- | xargs\")\n",
    "\n",
    "info = {\n",
    "    \"Timestamp\"        : datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"Host\"             : platform.node(),\n",
    "    \"OS\"               : f\"{platform.system()} {platform.release()}\",\n",
    "    \"Python\"           : platform.python_version(),\n",
    "    \"TensorFlow\"       : tf.__version__,\n",
    "    \"TensorFlow Build\" : tf.sysconfig.get_build_info().get(\"build_type\",\"N/A\"),\n",
    "    \"CUDA Built w/\"    : tf.sysconfig.get_build_info().get(\"cuda_version\",\"N/A\"),\n",
    "    \"cuDNN Built w/\"   : tf.sysconfig.get_build_info().get(\"cudnn_version\",\"N/A\"),\n",
    "    \"Num GPUs visible\" : len(tf.config.experimental.list_physical_devices('GPU')),\n",
    "    \"GPU Name(s)\"      : run(\"nvidia-smi --query-gpu=name --format=csv,noheader\"),\n",
    "    \"GPU Driver\"       : run(\"nvidia-smi --query-gpu=driver_version --format=csv,noheader | head -1\"),\n",
    "    \"CUDA Runtime\"     : run(\"nvcc --version | grep release | awk '{print $6}'\"),\n",
    "    \"cuDNN Runtime\"    : run(\"grep -oP 'CUDNN_MAJOR\\\\s*=\\\\s*\\\\K[0-9]+' /usr/include/cudnn_version.h 2>/dev/null\") + \".\" +\n",
    "                          run(\"grep -oP 'CUDNN_MINOR\\\\s*=\\\\s*\\\\K[0-9]+' /usr/include/cudnn_version.h 2>/dev/null\"),\n",
    "    \"CPU Model\"        : cpu_model,\n",
    "    \"CPU (logical)\"    : os.cpu_count(),\n",
    "    \"CPU (physical)\"   : os.cpu_count()//2 if os.cpu_count() else \"N/A\",\n",
    "    \"RAM (free/total)\" : run(\"free -h | awk '/Mem:/ {print $3\\\"/\\\"$2}'\")\n",
    "}\n",
    "\n",
    "print(\"\\n\".join(f\"{k:<16}: {v}\" for k,v in info.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47915f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  SHHS-1  4-Channel-Preprocessing\n",
    "import os, gc, re, pathlib, concurrent.futures as cf\n",
    "from itertools import islice, groupby\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import mne\n",
    "from pyedflib import EdfReader   # Fallback\n",
    "from lxml import etree\n",
    "\n",
    "WIN_SEC, STEP_SEC, MIN_APNEA_S = 60, 30, 10          # Sliding-Window Settings\n",
    "ORDER = [\"SAO2\", \"HR\", \"THOR RES\", \"ABDO RES\"]       # order in X\n",
    "ALIAS = {                                            # possible channel-synonyms\n",
    "    \"SAO2\":     [\"SAO2\", \"SPO2\", \"SATS\"],\n",
    "    \"HR\":       [\"H.R.\", \"HR\", \"PR\", \"PULSERATE\"],\n",
    "    \"THOR RES\": [\"THOR RES\", \"THOR\", \"CHEST\", \"THOR RIP\"],\n",
    "    \"ABDO RES\": [\"ABDO RES\", \"ABD\", \"ABD RIP\", \"ABDOMEN\"],\n",
    "}\n",
    "\n",
    "DATA_DIR = pathlib.Path(\"~/MSI-HTWG/Seminar/data/nsrr/shhs/polysomnography\").expanduser()\n",
    "EDF_DIR  = DATA_DIR / \"edfs/shhs1\"\n",
    "ANN_DIR  = DATA_DIR / \"annotations-events-nsrr/shhs1\"\n",
    "\n",
    "OUT_DIR  = pathlib.Path(\"~/MSI-HTWG/Seminar/blocks_4signale_1Hz_SHHS1\").expanduser()\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "subj_id = lambda p: p.name.split(\"-\")[1].split(\".\")[0]\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    \"\"\"unifies channel names (capitatlization, special characters)\"\"\"\n",
    "    return re.sub(r'[\\W_]+', '', s.upper())\n",
    "\n",
    "def find_ch(raw, keys):\n",
    "    \"\"\"find first available channel from list of aliases\"\"\"\n",
    "    up = {_norm(c): c for c in raw.ch_names}\n",
    "    for k in keys:\n",
    "        n = _norm(k)\n",
    "        if n in up:\n",
    "            return up[n]\n",
    "\n",
    "def apnea_events(xml_path):\n",
    "    \"\"\"returns (start, duration) of all relevant apnea events\"\"\"\n",
    "    ok = {\"obstructive apnea\", \"central apnea\", \"mixed apnea\", \"hypopnea\"}\n",
    "    for ev in etree.parse(str(xml_path)).findall(\".//ScoredEvent\"):\n",
    "        label = f\"{ev.findtext('EventType','').lower()}|\" \\\n",
    "                f\"{ev.findtext('EventConcept','').lower()}\"\n",
    "        if any(k in label for k in ok):\n",
    "            yield float(ev.findtext(\"Start\")), float(ev.findtext(\"Duration\"))\n",
    "\n",
    "def contig(a, n):\n",
    "    \"\"\"True, if >= n consecutive 1s in Binary vector\"\"\"\n",
    "    return any(len(list(g)) >= n for k, g in groupby(a) if k)\n",
    "\n",
    "# Fallback-Loader (pyEDFlib)\n",
    "def load_with_pyedflib(edf_path, alias_list):\n",
    "    \"\"\"reads EDF via pyEDFlib, resamples to 1 Hz\"\"\"\n",
    "    with EdfReader(str(edf_path)) as f:\n",
    "        lbl_up = {lbl.upper(): i for i, lbl in enumerate(f.getSignalLabels())}\n",
    "        idxs   = [next((lbl_up.get(k.upper()) for k in ks if k.upper() in lbl_up), None)\n",
    "                  for ks in alias_list]\n",
    "        if None in idxs:\n",
    "            return None\n",
    "\n",
    "        sfs    = f.getSampleFrequencies()\n",
    "        dur    = int(f.file_duration)\n",
    "        fs_max = int(max(sfs[i] for i in idxs))      # highest frequency\n",
    "        data   = []\n",
    "        for i in idxs:\n",
    "            sig = f.readSignal(i).astype(np.float32)\n",
    "            if len(sig) < fs_max * dur:              # fill up missing samples\n",
    "                sig = np.pad(sig, (0, fs_max * dur - len(sig)), mode='edge')\n",
    "            data.append(sig)\n",
    "\n",
    "        X_raw = np.stack(data, axis=1)               # (T_raw,4)\n",
    "        X_1hz = X_raw.reshape(-1, fs_max, 4).mean(1) # (T,4) @1 Hz\n",
    "        return X_1hz\n",
    "\n",
    "def process_one(edf_path):\n",
    "    sid   = subj_id(edf_path)\n",
    "    out_p = OUT_DIR / f\"block_{sid}.npz\"\n",
    "    if out_p.exists():\n",
    "        return \"skip\"\n",
    "\n",
    "    # MNE\n",
    "    try:\n",
    "        raw = mne.io.read_raw_edf(edf_path, preload=False, verbose='ERROR')\n",
    "        chs = [find_ch(raw, ALIAS[c]) for c in ORDER]\n",
    "        if None in chs:\n",
    "            return \"missing\"\n",
    "        raw.pick(chs).load_data()\n",
    "        raw.resample(1, npad=\"auto\")\n",
    "        X = raw.get_data().T.astype(np.float32)      # (T,4)\n",
    "    except AssertionError:\n",
    "        # pyEDFlib\n",
    "        X = load_with_pyedflib(edf_path, [ALIAS[c] for c in ORDER])\n",
    "        if X is None:\n",
    "            return \"missing\"\n",
    "    except Exception:\n",
    "        return \"corrupt\"\n",
    "\n",
    "    # Annotations\n",
    "    ann_file = ANN_DIR / f\"shhs1-{sid}-nsrr.xml\"\n",
    "    if not ann_file.exists():\n",
    "        return \"no_xml\"\n",
    "\n",
    "    y_sec = np.zeros(len(X), np.int8)\n",
    "    for s, d in apnea_events(ann_file):\n",
    "        y_sec[int(s): int(min(s + d, len(X)))] = 1\n",
    "\n",
    "    # Sliding window blocks\n",
    "    X_blk, y_blk = [], []\n",
    "    for t0 in range(0, len(X) - WIN_SEC + 1, STEP_SEC):\n",
    "        seg = X[t0:t0 + WIN_SEC]\n",
    "        seg = (seg - seg.mean(0)) / (seg.std(0) + 1e-8)\n",
    "        X_blk.append(seg.astype(np.float16))\n",
    "        y_blk.append(int(contig(y_sec[t0:t0 + WIN_SEC], MIN_APNEA_S)))\n",
    "\n",
    "    if X_blk:\n",
    "        np.savez_compressed(out_p,\n",
    "            X=np.stack(X_blk),\n",
    "            y=np.asarray(y_blk, np.int8),\n",
    "            subj_id=np.asarray([sid] * len(y_blk), '<U10'))\n",
    "    gc.collect()\n",
    "    return \"done\"\n",
    "\n",
    "all_edfs  = sorted(EDF_DIR.glob(\"shhs1-*.edf\"))\n",
    "done_ids  = {p.stem.split('_')[1] for p in OUT_DIR.glob(\"block_*.npz\")}\n",
    "todo_edfs = [p for p in all_edfs if subj_id(p) not in done_ids][:100]  # max. 100 new\n",
    "\n",
    "print(f\"Already loaded: {len(done_ids):>4} Blocks\")\n",
    "print(f\"TODO: {len(todo_edfs):>4} EDFs\\n\")\n",
    "\n",
    "N_WORKER = min(4, os.cpu_count() or 1)\n",
    "stats = {k: 0 for k in [\"done\", \"skip\", \"missing\", \"no_xml\", \"corrupt\"]}\n",
    "\n",
    "def chunked(it, n):\n",
    "    it = iter(it)\n",
    "    while (chunk := list(islice(it, n))):\n",
    "        yield chunk\n",
    "\n",
    "for bi, batch in enumerate(chunked(todo_edfs, 50), 1):\n",
    "    print(f\"Batch {bi} – {len(batch)} Dateien\")\n",
    "    with cf.ProcessPoolExecutor(max_workers=N_WORKER) as pool:\n",
    "        for res in tqdm(pool.map(process_one, batch), total=len(batch)):\n",
    "            stats[res] += 1\n",
    "\n",
    "print(\"\\Done .npz-Blöcke in:\", OUT_DIR)\n",
    "print(\"Summary:\", stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf0d807-4525-4ce3-991d-9d42cb2701c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model & SHHS-1 Data\n",
    "import pathlib, numpy as np, tensorflow as tf\n",
    "\n",
    "# Global Metrics\n",
    "ALPHA   = 0.10      # highest 10 % of points\n",
    "K_STEPS = 60        # number of steps for Deletion-/Insertion-AUC\n",
    "\n",
    "# load model\n",
    "MODEL_DIR = pathlib.Path(\"apnea_cnn_saved\")\n",
    "model = tf.keras.models.load_model(MODEL_DIR / \"net.keras\")\n",
    "\n",
    "# collect shhs1 blocks\n",
    "BLK_DIR   = pathlib.Path(\"~/MSI-HTWG/Seminar/blocks_4signale_1Hz_SHHS1\").expanduser()\n",
    "blk_files = sorted(BLK_DIR.glob(\"block_2*.npz\"))\n",
    "assert blk_files, f\"No SHHS-1-Blocks in {BLK_DIR}!\"\n",
    "\n",
    "X_list, y_list = [], []\n",
    "for f in blk_files:\n",
    "    d = np.load(f)\n",
    "    X_list.append(d[\"X\"].astype(\"float32\"))\n",
    "    y_list.append(d[\"y\"])\n",
    "X = np.concatenate(X_list, axis=0)    # (N, 60, 4)\n",
    "y = np.concatenate(y_list, axis=0)    # (N,)\n",
    "\n",
    "print(\"Windows :\", X.shape, \"| Apnea rate:\", y.mean().round(3))\n",
    "\n",
    "N_SAMPLES_VIS = 3\n",
    "rng           = np.random.default_rng()\n",
    "\n",
    "idx_apnea   = int(rng.choice(np.where(y == 1)[0]))\n",
    "\n",
    "other_idx   = rng.choice(\n",
    "    np.setdiff1d(np.arange(len(X)), [idx_apnea]),\n",
    "    size=N_SAMPLES_VIS - 1,\n",
    "    replace=False\n",
    ")\n",
    "\n",
    "samples_vis = [idx_apnea] + other_idx.tolist()\n",
    "\n",
    "print(\"Heatmap-Examples (idx):\", samples_vis,\n",
    "      \"| y_true:\", y[samples_vis])\n",
    "\n",
    "\n",
    "N_EVAL_SAMPLES = 100\n",
    "rng       = np.random.default_rng(42)\n",
    "idx_eval  = rng.choice(len(X), size=N_EVAL_SAMPLES, replace=False)\n",
    "X_eval    = X[idx_eval]\n",
    "print(\"Benchmark-samples:\", X_eval.shape)\n",
    "\n",
    "def random_attrib(shape: tuple[int, ...]) -> np.ndarray:\n",
    "    return np.random.randn(*shape).astype(\"float32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5a40cf-8610-4705-8f1d-908e9b9bf90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grad-CAM\n",
    "import matplotlib.pyplot as plt\n",
    "from tf_keras_vis.gradcam import Gradcam\n",
    "from tf_keras_vis.utils.model_modifiers import ReplaceToLinear\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.keras.backend.mean = tf.math.reduce_mean\n",
    "\n",
    "# find last conv1d-layer\n",
    "conv_layers  = [l for l in model.layers if isinstance(l, tf.keras.layers.Conv1D)]\n",
    "target_layer = conv_layers[-1]\n",
    "print(\"nutze Layer:\", target_layer.name)\n",
    "\n",
    "gradcam = Gradcam(model,\n",
    "                  model_modifier=ReplaceToLinear(),\n",
    "                  clone=True)\n",
    "\n",
    "# score-function\n",
    "score_fn = lambda out: out[:, 0]      # apnea score\n",
    "\n",
    "heatmaps = gradcam(score_fn,\n",
    "                   X[samples_vis],\n",
    "                   penultimate_layer=target_layer.name,\n",
    "                   seek_penultimate_conv_layer=False)\n",
    "\n",
    "for idx, hm in zip(samples_vis, heatmaps):\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    plt.title(f\"Grad-CAM idx={idx} y_true={y[idx]}\", fontsize=13)\n",
    "    plt.imshow(hm[np.newaxis, :], cmap='viridis',\n",
    "               aspect='auto', extent=[0, 60, 0, 1])\n",
    "    plt.xlabel(\"Second in 60-s-window\");  plt.yticks([])\n",
    "    plt.colorbar(fraction=0.03, pad=0.01)\n",
    "    plt.tight_layout();  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9946d6f-8c51-4411-8372-47a77176c932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrated Gradients\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def integrated_gradients(model,\n",
    "                         inputs,          # (N, 60, 4)\n",
    "                         baseline=None,\n",
    "                         steps=64,\n",
    "                         alpha_batch=128):\n",
    "\n",
    "    x = tf.convert_to_tensor(inputs, tf.float32)\n",
    "    N = tf.shape(x)[0]\n",
    "\n",
    "    if baseline is None:\n",
    "        baseline = tf.zeros_like(x)\n",
    "    else:\n",
    "        baseline = tf.convert_to_tensor(baseline, tf.float32)\n",
    "\n",
    "    alphas = tf.linspace(0., 1., steps + 1)  # (steps+1,)\n",
    "    ig_accum = tf.zeros_like(x)\n",
    "\n",
    "    for a0 in range(0, steps + 1, alpha_batch):\n",
    "        a1 = min(a0 + alpha_batch, steps + 1)\n",
    "        a = alphas[a0:a1]                       # (b,)\n",
    "        b = a1 - a0\n",
    "\n",
    "        a = a[:, None, None, None]              # (b,1,1,1)\n",
    "        inter = baseline[None] + a * (x - baseline)[None]   # (b,N,60,4)\n",
    "        inter = tf.reshape(inter, (b * N, 60, 4))           # (b·N,60,4)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(inter)\n",
    "            preds = model(inter, training=False)[:, 0]      # (b·N,)\n",
    "        grads = tape.gradient(preds, inter)                 # (b·N,60,4)\n",
    "\n",
    "        grads = tf.reshape(grads, (b, N, 60, 4))            # (b,N,60,4)\n",
    "        ig_accum += tf.reduce_mean(grads, axis=0)           # (N,60,4)\n",
    "\n",
    "    ig = (x - baseline) * ig_accum                         # (N,60,4)\n",
    "    return ig.numpy()\n",
    "\n",
    "baseline = np.zeros_like(X[samples_vis])\n",
    "igrads   = integrated_gradients(model,\n",
    "                                X[samples_vis],\n",
    "                                baseline=baseline,\n",
    "                                steps=64)     # (3,60,4)\n",
    "\n",
    "for idx, ig_arr in zip(samples_vis, igrads):\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    ŷ = model.predict(X[idx:idx+1], verbose=0)[0, 0]\n",
    "    plt.title(f\"Integrated Gradients  |  idx={idx}   ŷ={ŷ:.3f}\", fontsize=13)\n",
    "    plt.plot(ig_arr.mean(-1), lw=2)\n",
    "    plt.axhline(0, color='k', lw=.8)\n",
    "    plt.xlim(0, 59)\n",
    "    plt.xlabel(\"Second in 60-s-window\")\n",
    "    plt.ylabel(\"Attribution (Σ channels)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbb493b-0109-427b-9fb2-16db162a1df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep SHAP\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Explainer for demo\n",
    "bg   = X[np.random.choice(len(X), 100, replace=False)]\n",
    "shx  = shap.DeepExplainer(model, bg)\n",
    "raw  = shx.shap_values(X[samples_vis])[0]   # (3,60,4,4) oder (60,4,4)\n",
    "\n",
    "if raw.ndim == 4:\n",
    "    maps = raw.mean(-1)                     # (3,60,4)\n",
    "else:\n",
    "    maps = raw.mean(-1)[None, ...]          # (1,60,4)\n",
    "\n",
    "for idx, hm in zip(samples_vis, maps):\n",
    "    # hm.shape == (60,4)\n",
    "    hm1 = hm.mean(-1)[None, :]              # (1,60)\n",
    "\n",
    "    v = np.max(np.abs(hm1))\n",
    "    plt.figure(figsize=(10,2))\n",
    "    plt.title(f\"SHAP idx={idx} y_true={y[idx]}\", fontsize=13)\n",
    "    plt.imshow(hm1, cmap='coolwarm',\n",
    "               aspect='auto',\n",
    "               extent=[0,60,0,1],\n",
    "               vmin=-v, vmax= v)\n",
    "    plt.xlabel(\"Seconds in 60-s-window\")\n",
    "    plt.yticks([])\n",
    "    plt.colorbar(label=\"Attribution\", fraction=0.03, pad=0.01)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276bb6eb-bd7c-4cc2-adf5-3d1e950c6638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIME\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# build explainler\n",
    "lime_exp = lime_tab.LimeTabularExplainer(\n",
    "    training_data         = X.reshape(len(X), -1),\n",
    "    mode                  = 'classification',\n",
    "    class_names           = ['NoApnea', 'Apnea'],\n",
    "    discretize_continuous = False)\n",
    "\n",
    "def _model_predict(flat):\n",
    "    batch = flat.reshape(-1, 60, 4)\n",
    "    p1 = model.predict(batch, verbose=0)\n",
    "    return np.hstack([1 - p1, p1])\n",
    "\n",
    "# run explainer\n",
    "lime_maps = []\n",
    "for s in X[samples_vis]:\n",
    "    exp = lime_exp.explain_instance(s.flatten(),\n",
    "                                   predict_fn=_model_predict,\n",
    "                                   num_features=240)\n",
    "    w = np.zeros(240)\n",
    "    for idx_feat, val in exp.local_exp[1]:\n",
    "        w[idx_feat] = val\n",
    "    lime_maps.append(w.reshape(60,4))\n",
    "lime_maps = np.stack(lime_maps)    # (3,60,4)\n",
    "\n",
    "for idx, hm in zip(samples_vis, lime_maps):\n",
    "    plt.figure(figsize=(10,2))\n",
    "    plt.title(f\"LIME  idx={idx} y_true={y[idx]}\")\n",
    "    plt.imshow(hm.mean(-1)[None,:], cmap='viridis',\n",
    "               aspect='auto', extent=[0,60,0,1])\n",
    "    plt.xlabel(\"Second in 60-s-window\"); plt.yticks([])\n",
    "    plt.colorbar(fraction=0.03, pad=0.01)\n",
    "    plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3d68e7-462a-47b9-a776-77a22503e3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, shap, lime.lime_tabular as lime_tab, tensorflow as tf\n",
    "from tf_keras_vis.gradcam import Gradcam\n",
    "from tf_keras_vis.utils.model_modifiers import ReplaceToLinear\n",
    "from tf_explain.core.integrated_gradients import IntegratedGradients\n",
    "\n",
    "# Helper\n",
    "def _ensure_60_4(arr, n_chan=4):\n",
    "    arr = np.asarray(arr, dtype=\"float32\")\n",
    "\n",
    "    # remove trailing singleton-axe\n",
    "    if arr.ndim == 4 and arr.shape[-1] == 1:      # (N,60,4,1)\n",
    "        arr = arr[..., 0]                         # zu (N,60,4)\n",
    "    if arr.ndim == 3 and arr.shape[-1] == 1:      # (60,4,1)\n",
    "        arr = arr[..., 0]                         # zu (60,4)\n",
    "\n",
    "    # (1,N,60,4) zu (N,60,4)\n",
    "    if arr.ndim == 4 and arr.shape[0] == 1 and arr.shape[2:] == (60, n_chan):\n",
    "        arr = arr[0]\n",
    "    # (N,60) zu (N,60,4)\n",
    "    if arr.ndim == 2 and arr.shape[1] == 60:\n",
    "        arr = np.repeat(arr[..., None], n_chan, axis=-1)\n",
    "\n",
    "    # (60,4) → (1,60,4)\n",
    "    if arr.ndim == 2 and arr.shape == (60, n_chan):\n",
    "        arr = arr[None, ...]\n",
    "\n",
    "    # (N,4,60)  zu (N,60,4)\n",
    "    if arr.ndim == 3 and arr.shape[1:] == (n_chan, 60):\n",
    "        arr = arr.swapaxes(1, 2)\n",
    "\n",
    "    assert arr.ndim == 3 and arr.shape[2] == n_chan, f\"Shape‐Fehler: {arr.shape}\"\n",
    "    return arr\n",
    "\n",
    "# IG (tf-explain)\n",
    "ig_exp = IntegratedGradients()\n",
    "\n",
    "def map_igrads(x, steps=64, batch_size=32):\n",
    "    attrs = []\n",
    "    for i in range(0, len(x), batch_size):\n",
    "        xb = x[i:i+batch_size]\n",
    "        ig_raw = ig_exp.get_integrated_gradients(\n",
    "            interpolated_images=IntegratedGradients.generate_interpolations(xb, steps),\n",
    "            model=model,\n",
    "            class_index=0,\n",
    "            n_steps=steps,\n",
    "        )                       # (batch, 60, 4)\n",
    "        attrs.append(ig_raw)\n",
    "    ig = np.concatenate(attrs, axis=0)        # (N, 60, 4)\n",
    "    return _ensure_60_4(ig)\n",
    "\n",
    "# Grad-CAM\n",
    "pen_layer = next(l for l in reversed(model.layers)\n",
    "                 if isinstance(l, tf.keras.layers.Conv1D))\n",
    "gradcam = Gradcam(model, model_modifier=ReplaceToLinear(), clone=True)\n",
    "score_fn = lambda y: y[:, 0]\n",
    "\n",
    "def map_gradcam(x):\n",
    "    hm = gradcam(score_fn, x,\n",
    "                 penultimate_layer=pen_layer.name,\n",
    "                 seek_penultimate_conv_layer=False)   # (N,60)\n",
    "    return _ensure_60_4(hm)\n",
    "\n",
    "# SHAP\n",
    "bg   = X[np.random.choice(len(X), 100, replace=False)]\n",
    "shxp = shap.DeepExplainer(model, bg)\n",
    "\n",
    "def map_shap(x):\n",
    "    sv = shxp.shap_values(x)[0]          # evtl. (N,60,4)\n",
    "    return _ensure_60_4(sv)\n",
    "\n",
    "# LIME\n",
    "lime_exp = lime_tab.LimeTabularExplainer(\n",
    "    training_data=X.reshape(len(X), -1),\n",
    "    mode='classification',\n",
    "    class_names=['NoApnea', 'Apnea'],\n",
    "    discretize_continuous=False)\n",
    "\n",
    "def _model_predict(flat):\n",
    "    batch = flat.reshape(-1, 60, 4)\n",
    "    p1 = model.predict(batch, verbose=0)\n",
    "    return np.hstack([1 - p1, p1])\n",
    "\n",
    "def map_lime(x):\n",
    "    outs = []\n",
    "    for s in x:\n",
    "        exp = lime_exp.explain_instance(\n",
    "            s.flatten(), predict_fn=_model_predict, num_features=240)\n",
    "        w = np.zeros(240)\n",
    "        for idx, val in exp.local_exp[1]:\n",
    "            w[idx] = val\n",
    "        outs.append(w.reshape(60, 4))\n",
    "    return _ensure_60_4(np.stack(outs))\n",
    "\n",
    "methods = {\n",
    "    \"Grad-CAM\": map_gradcam,\n",
    "    \"IntGrad\" : map_igrads,\n",
    "    \"SHAP\"    : map_shap,\n",
    "    \"LIME\"    : map_lime,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9778f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_explain.core.integrated_gradients import IntegratedGradients\n",
    "ig_explain = IntegratedGradients()\n",
    "\n",
    "def map_igrads(x, steps=64):\n",
    "    # generate interpolations (N*(steps+1), 60, 4)\n",
    "    inter = IntegratedGradients.generate_interpolations(np.array(x), steps)\n",
    "\n",
    "    # raw IG-Score\n",
    "    ig_raw = IntegratedGradients.get_integrated_gradients(\n",
    "        interpolated_images  = inter,\n",
    "        model                = model,\n",
    "        class_index          = 0,\n",
    "        n_steps              = steps,\n",
    "    )                                   # (N, 60, 4)\n",
    "\n",
    "    return _ensure_60_4(ig_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79bd221",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_ig  = map_igrads(X_eval[:5])\n",
    "A_cam = map_gradcam(X_eval[:5])\n",
    "print(\"IG :\", A_ig.shape, A_ig.min(), A_ig.max())\n",
    "print(\"CAM:\", A_cam.shape, A_cam.min(), A_cam.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba4c612-f511-4530-9d1b-0fd8e5fdcd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _focus_mask_order(attrib):\n",
    "    \"\"\"returns sorted indexes (important to unimportant) and threshold\"\"\"\n",
    "    a = np.abs(attrib).mean(0)          # (60,4)\n",
    "    flat = np.argsort(-a, axis=None)    # decending\n",
    "    n_focus = int(ALPHA * flat.size)    # Top 10 percent\n",
    "    return flat[:n_focus], n_focus\n",
    "\n",
    "def deletion_auc(attrib, k_steps=K_STEPS):\n",
    "    flat, n_focus = _focus_mask_order(attrib)\n",
    "    masks = np.ones_like(X_eval, bool)\n",
    "    auc, prev = 0., model.predict(X_eval, verbose=0).mean()\n",
    "    step = max(1, n_focus // k_steps)\n",
    "    for k in range(0, n_focus, step):\n",
    "        blk = flat[k:k+step]\n",
    "        masks.reshape(-1)[blk] = False\n",
    "        conf = model.predict(X_eval * masks, verbose=0).mean()\n",
    "        auc += (prev + conf) / 2\n",
    "        prev = conf\n",
    "    return auc / (n_focus // step)\n",
    "\n",
    "def insertion_auc(attrib, k_steps=K_STEPS):\n",
    "    flat, n_focus = _focus_mask_order(attrib)\n",
    "    masks = np.zeros_like(X_eval, bool)\n",
    "    auc, prev = 0., model.predict(np.zeros_like(X_eval), verbose=0).mean()\n",
    "    step = max(1, n_focus // k_steps)\n",
    "    for k in range(0, n_focus, step):\n",
    "        blk = flat[k:k+step]\n",
    "        masks.reshape(-1)[blk] = True\n",
    "        conf = model.predict(X_eval * masks, verbose=0).mean()\n",
    "        auc += (prev + conf) / 2\n",
    "        prev = conf\n",
    "    return auc / (n_focus // step)\n",
    "\n",
    "def sparsity_entropy(attrib):\n",
    "    # attrib: (N,60,4)\n",
    "    p = np.abs(attrib).sum((1,2))\n",
    "    p = p / p.sum()\n",
    "    return -(p * np.log(p + 1e-9)).sum()\n",
    "\n",
    "def latency_ms(fn):\n",
    "    t0 = time.time()\n",
    "    fn(X_eval)\n",
    "    return (time.time() - t0) * 1000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2c8e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_STEPS  = 60\n",
    "MASK_VAL = -10.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e51ea6-c514-44d9-b9be-2f6e1dae55b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-Benchmark Full Run (4 Methods, Shape-Fix)\n",
    "import numpy as np, time, math\n",
    "from tqdm import tqdm\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "K_STEPS   = 60\n",
    "MASK_VAL  = -10.0\n",
    "N_EVAL    = 100          # Windows per iterations\n",
    "N_REPEAT  = 3            # Iterations\n",
    "\n",
    "# Eval-Subset\n",
    "yh_full  = model.predict(X, verbose=0)[:, 0]\n",
    "mid_idx = np.where((yh_full > .2) & (yh_full < .8))[0]\n",
    "\n",
    "# Shape & Mask helper\n",
    "to_3d  = lambda A: A[:, :, None] if A.ndim == 2 else A        # (B,60) → (B,60,1)\n",
    "def align_maps(A, Xb):\n",
    "    A = to_3d(A)\n",
    "    if A.shape[0] != Xb.shape[0]:\n",
    "        A = np.repeat(A, Xb.shape[0], axis=0)\n",
    "    return A\n",
    "def mask_ts(x, t_idx):\n",
    "    x = x.copy()\n",
    "    x[:, t_idx, :] = MASK_VAL\n",
    "    return x\n",
    "\n",
    "# Deletion-AUC\n",
    "def deletion_auc(A, Xb):\n",
    "    A = align_maps(A, Xb)\n",
    "    B, T, C = Xb.shape\n",
    "    order = np.argsort(A.reshape(B, -1), axis=1)[:, ::-1]\n",
    "    step  = max(1, order.shape[1] // K_STEPS)\n",
    "    aucs  = []\n",
    "    for b in range(B):\n",
    "        x_mod = Xb[b:b+1].copy()\n",
    "        base  = model.predict(x_mod, verbose=0)[0, 0] + 1e-8\n",
    "        scores = [1.0]\n",
    "        for i in range(0, order.shape[1], step):\n",
    "            tids = order[b, i:i+step] // C\n",
    "            x_mod = mask_ts(x_mod, tids)\n",
    "            scores.append(model.predict(x_mod, verbose=0)[0, 0] / base)\n",
    "        aucs.append(np.trapz(scores, dx=1/K_STEPS))\n",
    "    return np.mean(aucs)\n",
    "\n",
    "# Insertion-AUC\n",
    "def insertion_auc(A, Xb):\n",
    "    A = align_maps(A, Xb)\n",
    "    B, T, C = Xb.shape\n",
    "    order = np.argsort(A.reshape(B, -1), axis=1)[:, ::-1]\n",
    "    step  = max(1, order.shape[1] // K_STEPS)\n",
    "    aucs  = []\n",
    "    for b in range(B):\n",
    "        x_mod = np.full_like(Xb[b:b+1], MASK_VAL)\n",
    "        base  = model.predict(x_mod, verbose=0)[0, 0]\n",
    "        scores = [0.0]\n",
    "        for i in range(0, order.shape[1], step):\n",
    "            tids = order[b, i:i+step] // C\n",
    "            x_mod[:, tids, :] = Xb[b:b+1, tids, :]\n",
    "            scores.append((model.predict(x_mod, verbose=0)[0, 0] - base) /\n",
    "                          (1 - base + 1e-8))\n",
    "        aucs.append(np.trapz(scores, dx=1/K_STEPS))\n",
    "    return np.mean(aucs)\n",
    "\n",
    "# normalized Sparsity (0 … 1)\n",
    "def sparsity_norm(A):\n",
    "    p = np.abs(A).ravel()\n",
    "    p /= p.sum() + 1e-8\n",
    "    H = -(p * np.log2(p + 1e-12)).sum()\n",
    "    return (math.log2(A.size) - H) / math.log2(A.size)\n",
    "\n",
    "# Latency\n",
    "def lat_ms(f, X1):\n",
    "    f(X1)\n",
    "    t0 = time.perf_counter_ns()\n",
    "    _  = f(X1)\n",
    "    return (time.perf_counter_ns() - t0) / 1_000_000\n",
    "\n",
    "all_methods = {\n",
    "    \"Grad-CAM\": methods[\"Grad-CAM\"],\n",
    "    \"IntGrad\" : methods[\"IntGrad\"],\n",
    "    \"SHAP\"    : methods[\"SHAP\"],\n",
    "    \"LIME\"    : methods[\"LIME\"],\n",
    "    \"Random\"  : lambda x: rng.standard_normal(x.shape)\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, func in all_methods.items():\n",
    "    del_s, ins_s, spa_s, lat_s = [], [], [], []\n",
    "    for _ in tqdm(range(N_REPEAT), desc=f\"{name} runs\"):\n",
    "        idx   = rng.choice(mid_idx, size=N_EVAL, replace=False)\n",
    "        X_ev  = X[idx]\n",
    "        A_map = func(X_ev)\n",
    "\n",
    "        del_s.append(deletion_auc(A_map, X_ev))\n",
    "        ins_s.append(insertion_auc(A_map, X_ev))\n",
    "        spa_s.append(sparsity_norm(A_map))\n",
    "        lat_s.append(0.0 if name == \"Random\" else lat_ms(func, X_ev[:1]))\n",
    "    results.append(dict(\n",
    "        method   = name,\n",
    "        del_auc  = np.mean(del_s),\n",
    "        ins_auc  = np.mean(ins_s),\n",
    "        sparsity = np.mean(spa_s),\n",
    "        lat_ms   = np.mean(lat_s)\n",
    "    ))\n",
    "\n",
    "print(f\"\\n=== Benchmark ({N_REPEAT}x{N_EVAL} Windows | {K_STEPS} Steps) ===\")\n",
    "for r in results:\n",
    "    print(f\"{r['method']:<8}  Del↓ {r['del_auc']:.3f}  \"\n",
    "          f\"Ins↑ {r['ins_auc']:.3f}  \"\n",
    "          f\"Spar↑ {r['sparsity']:.3f}  \"\n",
    "          f\"Lat {r['lat_ms']:.0f} ms\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
